# -*- coding: utf-8 -*-
"""Stance Detection in 280 Characters or Less.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CL1N0yGzdxyhXelbetjt0ghyShcvsMpe

The **Stance Detection in 280 Characters or Less** Project Notebook.

This project explores deep and classical architectures on the problem of stance detection. The [COVID-CQ dataset](https://github.com/eceveco/COVID-CQ) is used.


---


It was decided that in order to optimize code reuese without resorting to an external code service like Github, all models would be combined into the same notebook.

This notebook is structured as follows:
1. Preprocessing
2. SVM Experiment
3. BERT Experiment
4. XLNet Experiment
"""

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

from google.colab import drive
drive.mount('/content/drive')

# A dependency of the preprocessing for BERT inputs
!pip install -q -U tensorflow-text

!pip install -q tf-models-official

import os
import shutil

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer

import matplotlib.pyplot as plt

tf.get_logger().setLevel('ERROR')

import pandas as pd
import numpy as np

# Make numpy values easier to read.
np.set_printoptions(precision=3, suppress=True)

from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing

pd.set_option('display.max_colwidth',400)

"""Load Dataset"""

raw_df = pd.read_csv('/content/drive/MyDrive/Stance_combined.csv', usecols = ['full_text', 'stance'])

raw_df.head()

raw_df = raw_df.dropna()
raw_df = raw_df.drop_duplicates()
len(raw_df)
raw_df['stance'] = raw_df.stance.astype(int)
raw_df['full_text'] = raw_df.full_text.astype(str)
raw_df.dtypes

raw_df.columns

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from nltk.tokenize import RegexpTokenizer

"""The line below was toggled on and off to handle stemming comparisons.
> word = ps.stem(word)
"""

filtered_df = pd.DataFrame(columns=['Rating', 'Tweet'])

nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))
tokenizer = RegexpTokenizer(r'\w+')
ps = PorterStemmer()

for i in range(raw_df.shape[0]):
  try:
    word_tokens = tokenizer.tokenize(raw_df.at[i, 'full_text'])
  except:
    continue
  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]
  final_tweet = ''
  for word in filtered_sentence:
    word = word.lower()
    word = ps.stem(word)
    final_tweet += word + ' '
  filtered_df = filtered_df.append({'Rating':raw_df.at[i, 'stance'], 'Tweet':final_tweet}, ignore_index=True)

filtered_df.groupby('Rating').describe()

"""Test Train Split"""

from sklearn.model_selection import train_test_split

r_state = 57

X_trainval, X_test, y_trainval, y_test = train_test_split(filtered_df['Tweet'],filtered_df['Rating'], test_size = 0.2, stratify=filtered_df['Rating'], random_state = r_state)
X_train, X_val, y_train, y_val = train_test_split(X_trainval,y_trainval, test_size = 0.25, stratify=y_trainval, random_state = r_state)

"""**Classical Machine Learning - SVM**

To evaluate the merits of using a deep learning model over a classical one, we implemented a Bag Of Words + SVM approach to compare results to. The way this works is tweets are first vectorized into counts of the most frequent words from the training set. Then these sparse vectors are used to train and evaluate an SVM.

Clone the data so that other models aren't disrupted.
"""

X_trainval, X_test, y_trainval, y_test = X_trainval.copy(), X_test.copy(), y_trainval.copy().astype('int32'), y_test.copy().astype('int32')
X_train, X_val, y_train, y_val = X_train.copy(), X_val.copy(), y_train.copy(), y_val.copy()

"""Preprocess the data for the SVM. Encode the list of tweets into a list of bag-of-words representations so that the SVM has a limited set of features to operate on."""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.svm import SVC

class BagOfWordsEncoder(BaseEstimator, TransformerMixin):
  def __init__(self, vocabulary_size=500):
    self.vectorizer = None
    self.vocabulary_size = vocabulary_size
  def fit(self, x_dataset, y_dataset):
    self.vectorizer = CountVectorizer(max_features=self.vocabulary_size)
    self.vectorizer.fit(x_dataset)
    return self
  def transform(self, x_dataset, y_dataset=None):
     encoded_data = self.vectorizer.transform(x_dataset).toarray()
     # refs https://colab.research.google.com/github/littlecolumns/ds4j-notebooks/blob/master/text-analysis/notebooks/Counting%20words%20with%20scikit-learn's%20CountVectorizer.ipynb#scrollTo=70FLsLFeq4nW
     new_df = pd.DataFrame(encoded_data, columns=self.vectorizer.get_feature_names_out())
     if y_dataset != None:
      return new_df, y_dataset
     else:
      return new_df

def create_svm_pipeline(vocabulary_size=500, C=1, kernel='linear'):
  return Pipeline(steps=[
      ('encoder', BagOfWordsEncoder(vocabulary_size=vocabulary_size)),
      ('clf', SVC(C=C, kernel=kernel)),
  ])

BagOfWordsEncoder().fit_transform(
    [
     'ezriderso gatewaypundit dr geoffrey cli honest obgyn add inform hydroxychloroquin azithromycin treatment covid 19 seen go viral twitter mysteri http co modefdptrk',
     'good uk begin patient trial remdesivir treat coronaviru good reason begin similar trial chloroquin azithromycin much cheaper use franc usa http co npexfcbkzl',
     'repfarnesworth realdonaldtrump mike_p foxandfriend say cdc knew chloroquin may inhibitor corona viru 2005 lol',
     'someon biomedicin degre found interest common malaria amp rheumatoid arthriti drug hydroxychloroquin may prove tive affect least coronaviru patient test sampl small could lead major breakthrough',
    ],
    [0, 0, 0, 0]
  )

"""Hyperparameter search for the most accurate SVM model."""

from sklearn.model_selection import GridSearchCV

data_subset_len = 1000
clf = GridSearchCV(estimator=create_svm_pipeline(), cv=3, param_grid={'encoder__vocabulary_size': [10, 100, 500, 1000], 'clf__C': [1, 10], 'clf__kernel': ('linear', 'poly', 'rbf')})
clf.fit(X_trainval[:data_subset_len], y_trainval[:data_subset_len])
print(clf.cv_results_)
print(clf.best_params_)

"""Use the best model and report accuracy on the test set."""

best_svm = clf.best_estimator_
best_svm.fit(X_trainval, y_trainval)
best_svm.score(X_test, y_test)

"""**End Of Classical Machine Learning - SVM**

**Deep Learning - BERT**

In order to accomplish the primary goal of our project - to achieve an accurate deep learning model - a Bert implementation has been used. Words are encoded using the preencoder of the Bert model. Results are evaluated from the test set per standard practice.
"""

from sklearn.utils import class_weight
class_weights = class_weight.compute_class_weight('balanced',classes = np.unique(y_train),y = y_train)

y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
y_val = tf.keras.utils.to_categorical(y_val)

"""Class imbalance"""

class_weights
class_weight_dict = dict(enumerate(class_weights))
class_weight_dict

"""Load BERT"""

# bert_preprocessor = hub.KerasLayer(
#     "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
# bert_encoder = hub.KerasLayer(
#     "https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4",
#     trainable=True)

bert_preprocessor = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3")
bert_encoder = hub.KerasLayer(
    "https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4",
    trainable=True)

def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  encoder_inputs = bert_preprocessor(text_input)

  outputs = bert_encoder(encoder_inputs)
  # net = outputs['pooled_output']
  net = outputs['sequence_output']
  net = tf.keras.layers.Flatten()(net)
  # net = tf.keras.layers.Dense(512, activation='relu')(net)
  net = tf.keras.layers.Dropout(0.1)(net)
  # net = tf.keras.layers.Dense(128, activation='relu')(net)
  net = tf.keras.layers.Dense(3, activation=None, name='classifier')(net)
  return tf.keras.Model(text_input, net)

classifier_model = build_classifier_model()
tf.keras.utils.plot_model(classifier_model, "info.png", show_shapes=True)

classifier_model.summary()

loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
metrics = tf.metrics.CategoricalAccuracy()

callback = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=2, restore_best_weights=True)

epochs = 50
steps_per_epoch = len(X_train)
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)

init_lr = 2e-5
optimizer = optimization.create_optimizer(init_lr=init_lr,
                                          num_train_steps=num_train_steps,
                                          num_warmup_steps=num_warmup_steps,
                                          optimizer_type='adamw')

classifier_model.compile(optimizer=optimizer,
                         loss=loss,
                         metrics=metrics)

history = classifier_model.fit(X_train,
                               y_train,
                               validation_data = (X_val, y_val),
                               epochs=epochs,
                               callbacks=[callback])
                              #  class_weight = class_weight_dict)

loss, accuracy = classifier_model.evaluate(X_test, y_test)

print(f'Loss: {loss}')
print(f'Accuracy: {accuracy}')

"""Plot the accuracy and loss over time"""

history_dict = history.history
print(history_dict.keys())

acc = history_dict['categorical_accuracy']
val_acc = history_dict['val_categorical_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)
fig = plt.figure(figsize=(10, 6))
fig.tight_layout()

plt.subplot(2, 1, 1)
# "bo" is for "blue dot"
plt.plot(epochs, loss, 'r', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
# plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(2, 1, 2)
plt.plot(epochs, acc, 'r', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

y_pred = classifier_model.predict(X_test)
y_test_preds = [np.argmax(x) for x in y_pred]
y_test_ground = [np.argmax(x) for x in y_test]

import sklearn

print(sklearn.metrics.classification_report(y_test_ground, y_test_preds, digits = 4))

"""Save the Model"""

classifier_model.save('/content/drive/MyDrive/Stance Detection/models/BERT covid' + str(r_state))

saved_classifier = tf.keras.models.load_model('/content/drive/MyDrive/Stance Detection/models/BERT covid' + str(r_state), custom_objects={'AdamWeightDecay': optimizer})

loss, accuracy = saved_classifier.evaluate(X_test, y_test)

print(f'Loss: {loss}')
print(f'Accuracy: {accuracy}')

"""**End of Deep Learning - BERT**

**Deep Learning - XLNet**

Another deep learning model is used to test the accuracy of the Bert section and/or replace the Bert model for final results. This model is XLNet, which is a tuning of Bert designed to improve on some of its shortcomings. Similarly to Bert, tweets are encoded using an XLNet preprocessor (XLNetTokenizer) and evaluated on the test set.
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns

import nltk
import re


from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve

!pip install transformers
!pip install sentencepiece

plt.style.use('seaborn')

from transformers import TFXLNetModel, XLNetTokenizer

xlnet_model = 'xlnet-base-cased'
xlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)

def create_xlnet(mname):
    """ Creates the model. It is composed of the XLNet main block and then
    a classification head its added
    """
    # Define token ids as inputs
    word_inputs = tf.keras.Input(shape=(120,), name='word_inputs', dtype='int32')

    # Call XLNet model
    xlnet = TFXLNetModel.from_pretrained(mname)
    xlnet_encodings = xlnet(word_inputs)[0]

    # CLASSIFICATION HEAD
    # Collect last step from last hidden state (CLS)
    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)
    # Apply dropout for regularization
    doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)
    # Final output
    outputs = tf.keras.layers.Dense(3, activation='softmax', name='outputs')(doc_encoding)

    # Compile model
    model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])
    model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=tf.metrics.CategoricalAccuracy())

    return model

xlnet = create_xlnet(xlnet_model)

xlnet.summary()

def get_inputs(tweets, tokenizer, max_len=120):
    """ Gets tensors from text using the tokenizer provided"""
    inputs = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in tweets]
    tokenized_inputs = np.array([a['input_ids'] for a in inputs])
    return tokenized_inputs

# def warmup(epoch, lr):
#     """Used for increasing the learning rate slowly, this tends to achieve better convergence.
#     However, as we are finetuning for few epoch it's not crucial.
#     """
#     return max(lr +1e-6, 2e-5)

def plot_metrics(pred, true_labels):
    """Plots a ROC curve with the accuracy and the AUC"""
    acc = accuracy_score(true_labels, np.array(pred.flatten() >= .5, dtype='int'))
    fpr, tpr, thresholds = roc_curve(true_labels, pred)
    auc = roc_auc_score(true_labels, pred)

    fig, ax = plt.subplots(1, figsize=(8,8))
    ax.plot(fpr, tpr, color='red')
    ax.plot([0,1], [0,1], color='black', linestyle='--')
    ax.set_title(f"AUC: {auc}\nACC: {acc}");
    return fig

r_state = 57
X_trainval, X_test, y_trainval, y_test = train_test_split(filtered_df['Tweet'],filtered_df['Rating'], test_size = 0.2, stratify=filtered_df['Rating'], random_state = r_state)
X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size = 0.15, random_state = r_state)

y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)
y_val = tf.keras.utils.to_categorical(y_val)

"""Training"""

callback = tf.keras.callbacks.EarlyStopping(monitor='val_categorical_accuracy', patience=2, restore_best_weights=True)

X_train = get_inputs(X_train, xlnet_tokenizer)
X_val = get_inputs(X_val, xlnet_tokenizer)

hist = xlnet.fit(x=X_train, y=y_train, validation_data = (X_val, y_val), epochs=15, callbacks=[callback])

xlnet.save('/content/drive/MyDrive/ML/XLNet_covid' + str(r_state))
#saved_classifier = tf.keras.models.load_model('/content/drive/MyDrive/ML/XLNet_covid' + str(r_state))
#loss, accuracy = saved_classifier.evaluate(X_test, y_test)

"""Testing"""

X_test = get_inputs(X_test, xlnet_tokenizer)
preds = xlnet.predict(X_test, verbose=True)

loss, accuracy = xlnet.evaluate(X_test, y_test)

print(preds.size)
print(y_test.size)
print(f'Loss: {loss}')
print(f'Accuracy: {accuracy}')

plot_metrics(preds, y_test);

"""**End of Deep Learning - XLNet**"""